{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/nfl-big-data-bowl-2020/train.csv\n",
      "/kaggle/input/nfl-big-data-bowl-2020/kaggle/competitions/nflrush/test.csv.encrypted\n",
      "/kaggle/input/nfl-big-data-bowl-2020/kaggle/competitions/nflrush/sample_submission.csv.encrypted\n",
      "/kaggle/input/nfl-big-data-bowl-2020/kaggle/competitions/nflrush/__init__.py\n",
      "/kaggle/input/nfl-big-data-bowl-2020/kaggle/competitions/nflrush/competition.cpython-36m-x86_64-linux-gnu.so\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import xgboost as xgb\n",
    "import keras\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn import metrics\n",
    "import math\n",
    "from string import punctuation\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# Any results you write to the current directory are saved as output.\n",
    "from kaggle.competitions import nflrush"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "# You can only call make_env() once, so don't lose it!\n",
    "env = nflrush.make_env()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('/kaggle/input/nfl-big-data-bowl-2020/train.csv', low_memory=False)\n",
    "#train.head(23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean windspeed\n",
    "train['WindSpeed'] = train['WindSpeed'].apply(lambda x: x.lower().replace('mph', '').strip() if not pd.isna(x) else x)\n",
    "train['WindSpeed'] = train['WindSpeed'].apply(lambda x: (int(x.split('-')[0])+int(x.split('-')[1]))/2 if not pd.isna(x) and '-' in x else x)\n",
    "train['WindSpeed'] = train['WindSpeed'].apply(lambda x: (int(x.split()[0])+int(x.split()[-1]))/2 if not pd.isna(x) and type(x)!=float and 'gusts up to' in x else x)\n",
    "    \n",
    "def str_to_float(txt):\n",
    "    try:\n",
    "        return float(txt)\n",
    "    except:\n",
    "        return -1\n",
    "    \n",
    "train['WindSpeed'] = train['WindSpeed'].apply(str_to_float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean wind direction\n",
    "def clean_WindDirection(txt):\n",
    "    if pd.isna(txt):\n",
    "        return np.nan\n",
    "    txt = txt.lower()\n",
    "    txt = ''.join([c for c in txt if c not in punctuation])\n",
    "    txt = txt.replace('from', '')\n",
    "    txt = txt.replace(' ', '')\n",
    "    txt = txt.replace('north', 'n')\n",
    "    txt = txt.replace('south', 's')\n",
    "    txt = txt.replace('west', 'w')\n",
    "    txt = txt.replace('east', 'e')\n",
    "    return txt\n",
    "\n",
    "train['WindDirection'] = train['WindDirection'].apply(clean_WindDirection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_WindDirection(txt):\n",
    "    if pd.isna(txt):\n",
    "        return np.nan\n",
    "    \n",
    "    if txt=='n':\n",
    "        return 0\n",
    "    if txt=='nne' or txt=='nen':\n",
    "        return 1/8\n",
    "    if txt=='ne':\n",
    "        return 2/8\n",
    "    if txt=='ene' or txt=='nee':\n",
    "        return 3/8\n",
    "    if txt=='e':\n",
    "        return 4/8\n",
    "    if txt=='ese' or txt=='see':\n",
    "        return 5/8\n",
    "    if txt=='se':\n",
    "        return 6/8\n",
    "    if txt=='ses' or txt=='sse':\n",
    "        return 7/8\n",
    "    if txt=='s':\n",
    "        return 8/8\n",
    "    if txt=='ssw' or txt=='sws':\n",
    "        return 9/8\n",
    "    if txt=='sw':\n",
    "        return 10/8\n",
    "    if txt=='sww' or txt=='wsw':\n",
    "        return 11/8\n",
    "    if txt=='w':\n",
    "        return 12/8\n",
    "    if txt=='wnw' or txt=='nww':\n",
    "        return 13/8\n",
    "    if txt=='nw':\n",
    "        return 14/8\n",
    "    if txt=='nwn' or txt=='nnw':\n",
    "        return 15/8\n",
    "    return np.nan\n",
    "\n",
    "train['WindDirection'] = train['WindDirection'].apply(transform_WindDirection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean Turf\n",
    "Turf = {'Field Turf':'Artificial', 'A-Turf Titan':'Artificial', 'Grass':'Natural', 'UBU Sports Speed S5-M':'Artificial', \n",
    "        'Artificial':'Artificial', 'DD GrassMaster':'Artificial', 'Natural Grass':'Natural', \n",
    "        'UBU Speed Series-S5-M':'Artificial', 'FieldTurf':'Artificial', 'FieldTurf 360':'Artificial', 'Natural grass':'Natural', 'grass':'Natural', \n",
    "        'Natural':'Natural', 'Artifical':'Artificial', 'FieldTurf360':'Artificial', 'Naturall Grass':'Natural', 'Field turf':'Artificial', \n",
    "        'SISGrass':'Artificial', 'Twenty-Four/Seven Turf':'Artificial', 'natural grass':'Natural'} \n",
    "\n",
    "train['Turf'] = train['Turf'].map(Turf)\n",
    "train['Turf'] = train['Turf'] == 'Natural'\n",
    "\n",
    "# solve team name encoding problem\n",
    "map_abbr = {'ARI': 'ARZ', 'BAL': 'BLT', 'CLE': 'CLV', 'HOU': 'HST'}\n",
    "for abb in train['PossessionTeam'].unique():\n",
    "    map_abbr[abb] = abb\n",
    "    \n",
    "train['PossessionTeam'] = train['PossessionTeam'].map(map_abbr)\n",
    "train['HomeTeamAbbr'] = train['HomeTeamAbbr'].map(map_abbr)\n",
    "train['VisitorTeamAbbr'] = train['VisitorTeamAbbr'].map(map_abbr)\n",
    "\n",
    "# Before pivot:\n",
    "# Creat: IsBallCarrier, ToLeft, std_x, std_y, offense, age, bmi, player number\n",
    "\n",
    "#train['IsBallCarrier'] = train['NflId'] == train['NflIdRusher']\n",
    "train['ToLeft'] = train['PlayDirection'] == 'left'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_offense(df):\n",
    "    offense = []\n",
    "    for i in range(0,len(df)):\n",
    "        if df['HomeTeamAbbr'][i] == df['PossessionTeam'][i]:\n",
    "            if df['Team'][i] == 'home':\n",
    "                offense.append('offense')\n",
    "            else:\n",
    "                offense.append('defense')\n",
    "        else:\n",
    "            if df['Team'][i] == 'away':\n",
    "                offense.append('offense')\n",
    "            else:\n",
    "                offense.append('defense')\n",
    "    df['Offense'] = np.array(offense)\n",
    "    return df\n",
    "\n",
    "train = define_offense(train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bye_XY(df):\n",
    "    # 计算出centroid的坐标\n",
    "    x = df.groupby(['PlayId','Team'],as_index=False)['X'].mean()\n",
    "    x.columns = ['PlayId','Team','x_cen']\n",
    "    df = pd.merge(df,x,how=\"inner\",on=['PlayId','Team'])\n",
    "    \n",
    "    y = df.groupby(['PlayId','Team'],as_index=False)['Y'].mean()\n",
    "    y.columns = ['PlayId','Team','y_cen']\n",
    "    df = pd.merge(df,y,how=\"inner\",on=['PlayId','Team'])\n",
    "    \n",
    "    # 计算两点(当前球员和centroid)之间的距离\n",
    "    distances = []\n",
    "    for i in range(len(df)) : \n",
    "      x1 = df.loc[i, \"X\"]\n",
    "      y1 = df.loc[i, \"Y\"] \n",
    "      x2 = df.loc[i,\"x_cen\"]\n",
    "      y2 = df.loc[i,\"y_cen\"]\n",
    "      dis_i = math.sqrt((x2 - x1)**2 + (y2 - y1)**2)\n",
    "      distances.append(dis_i)\n",
    "    \n",
    "    df['distances'] = distances\n",
    "    \n",
    "    # 计算Average distance to centroid\n",
    "    avg_distance = df.groupby(['PlayId','Team'],as_index=False)['distances'].mean()\n",
    "    avg_distance.columns = ['PlayId','Team','avg_distance']\n",
    "    df = pd.merge(df,avg_distance,how=\"inner\",on=['PlayId','Team'])\n",
    "    \n",
    "#    # 计算qb的位置坐标\n",
    "    qb_pos = df[df[\"Position\"] == 'QB']\n",
    "    qb_pos = qb_pos[['PlayId','X','Y']]\n",
    "    qb_pos.columns = ['PlayId','qb_x','qb_y']\n",
    "    qb_pos.drop_duplicates(subset =\"PlayId\",keep = 'first',inplace = True)\n",
    "    df = pd.merge(df,qb_pos,how=\"left\",on=['PlayId'])\n",
    "    \n",
    "    # 计算球员和qb之间的距离\n",
    "    distances_qb = []\n",
    "    for i in range(len(df)) : \n",
    "      x1 = df.loc[i, \"X\"]\n",
    "      y1 = df.loc[i, \"Y\"] \n",
    "      x2 = df.loc[i,\"qb_x\"]\n",
    "      y2 = df.loc[i,\"qb_y\"]\n",
    "      dis_i = math.sqrt((x2 - x1)**2 + (y2 - y1)**2)\n",
    "      distances_qb.append(dis_i)\n",
    "    \n",
    "    df['distances_to_qb'] = distances_qb\n",
    "    \n",
    "    #计算Average distance to QB\n",
    "    avg_distance_to_qb = df.groupby(['PlayId','Team'],as_index=False)['distances_to_qb'].mean()\n",
    "    avg_distance_to_qb.columns = ['PlayId','Team','avg_distance_to_qb']\n",
    "    df = pd.merge(df,avg_distance_to_qb,how=\"inner\",on=['PlayId','Team'])\n",
    "    return df\n",
    "\n",
    "\n",
    "train = bye_XY(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Clean_Op(df):\n",
    "    Op=df.groupby('PlayId',as_index=False).agg({'OffensePersonnel':'first'})\n",
    "    Op_split=Op.OffensePersonnel.str.split(',',expand=True)\n",
    "    Op_split.columns=[\"s1\",\"s2\",\"s3\",\"s4\",\"s5\"]\n",
    "    #pivot s1\n",
    "    p_s1=Op_split.s1.str.split(' ',expand=True)\n",
    "    p_s1.columns=['number','position']\n",
    "    Op=p_s1.pivot(columns='position',values='number')\n",
    "    Op['PlayId']=Offense_personal['PlayId']\n",
    "    #pivot s2-s4\n",
    "    columns=list(Op_split)\n",
    "    columns=columns[1:4]\n",
    "    for i in columns:\n",
    "        new=Op_split[i].str.split(' ',expand=True)\n",
    "        new=new.drop(new.columns[0], axis=1)\n",
    "        new.columns=['number','position']\n",
    "        temp=new.pivot(columns='position',values='number')\n",
    "        temp['PlayId']=Offense_personal['PlayId']\n",
    "        Op=Op.merge(temp,on='PlayId',suffixes=('_left', '_right'))\n",
    "    #pivot s5\n",
    "    s5=Op_split.s5.str.split(' ',expand=True)\n",
    "    s5.columns=['number','position']\n",
    "    temp=s5.pivot(columns='position',values='number')\n",
    "    temp['PlayId']=Offense_personal['PlayId']\n",
    "    temp=temp.drop(temp.columns[0],axis=1)\n",
    "    Op=Op.merge(temp,on='PlayId',suffixes=('_left', '_right'))\n",
    "    #Cleaning the data frame\n",
    "    Op=Op.replace({np.nan: 0})\n",
    "    Op=Op.drop([np.nan],axis=1)\n",
    "    Op=Op.apply(pd.to_numeric)\n",
    "    Op['RB']=Op['RB_left']+Op['RB_right']\n",
    "    Op['TE']=Op['TE_left']+Op['TE_right']\n",
    "    Op['WR']=Op['WR_left']+Op['WR_right']\n",
    "    Op=Op.drop(['RB_left','RB_right'],axis=1)\n",
    "    Op=Op.drop(['TE_left','TE_right'],axis=1)\n",
    "    Op=Op.drop(['WR_left','WR_right'],axis=1)\n",
    "    \n",
    "    return Op\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 调试模块\n",
    "#import copy\n",
    "#df = copy.deepcopy(train)\n",
    "#df = Clean_Op(df)\n",
    "#df = dis_max(df)\n",
    "#df.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dis_max(df):\n",
    "    team_Max_distance=df.groupby(['PlayId','Team'],as_index=False).agg({'X':['max','min'],'Y':['max','min']})\n",
    "    team_Max_distance.columns=['PlayId','Team','X_max','X_min','Y_max','Y_min']\n",
    "    team_Max_distance.head()\n",
    "    team_Max_distance['max_X_distance']=team_Max_distance['X_max']-team_Max_distance['X_min']\n",
    "    team_Max_distance['max_Y_distance']=team_Max_distance['Y_max']-team_Max_distance['Y_min']\n",
    "    team_Max_distance2=team_Max_distance[['PlayId','Team','max_X_distance','max_Y_distance']]\n",
    "    df=pd.merge(df,team_Max_distance2,how=\"inner\",on=['PlayId','Team'])\n",
    "    return df\n",
    "\n",
    "train = dis_max(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_team_average_age(df):\n",
    "    a2 = pd.to_datetime(df['PlayerBirthDate']).dt.year\n",
    "    a3 = pd.to_datetime(df['TimeHandoff']).dt.year\n",
    "    a4 = a3-a2\n",
    "    df['age'] = np.array(a4)\n",
    "    team_average_age=df.groupby(['PlayId','Team'],as_index=False)['age'].mean()\n",
    "    team_average_age.columns=['PlayId','Team','team_avg_age']\n",
    "    df=pd.merge(df,team_average_age,how=\"inner\",on=['PlayId','Team'])\n",
    "    df=df.drop('age',axis=1)\n",
    "    return df\n",
    "\n",
    "train = define_team_average_age(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_bmi(df):\n",
    "    df['PlayerHeight'] = df['PlayerHeight'].apply(lambda x: 12*int(x.split('-')[0])+int(x.split('-')[1]))\n",
    "    df['PlayerBMI'] = 703*(df['PlayerWeight']/(df['PlayerHeight'])**2)\n",
    "    df=df.drop(['PlayerHeight','PlayerWeight'],axis=1)\n",
    "    return df\n",
    "\n",
    "# 给每个队编号，有可能方便后面pivot\n",
    "def append_player_number(df):\n",
    "    player_num = []\n",
    "    for i in range(0,len(df)):\n",
    "        if i+1 <= 11:\n",
    "            player_num.append(i+1)\n",
    "        else:\n",
    "            player_num.append(i%11+1)\n",
    "    df['player_num'] = np.array(player_num)\n",
    "\n",
    "train = define_bmi(train)\n",
    "append_player_number(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_Top10UniversityAlumni(df):\n",
    "    #According to Pro-football-reference \n",
    "    Top10University = [\"Notre Dame\",\"USC\",\"Ohio State\",\"Penn State\",\"Michigan\",\"Nebraska\",\"Oklahoma\",\"Alabama\",\"Miami\"]\n",
    "    gg=[]\n",
    "    for i in df['PlayerCollegeName']:\n",
    "        if i in Top10University:\n",
    "            gg.append(1)\n",
    "        else:\n",
    "            gg.append(0)\n",
    "    df['Alumni'] = gg\n",
    "    GroupTop10U = df.groupby(['PlayId','Team'],as_index=False).agg({'Alumni':['sum']})\n",
    "    GroupTop10U.columns=['PlayId','Team','SumTop10UniversityAlumni']\n",
    "    df=pd.merge(df,GroupTop10U,how=\"inner\",on=['PlayId','Team'])\n",
    "    df=df.drop('Alumni',axis=1)\n",
    "    return df\n",
    "\n",
    "train = define_Top10UniversityAlumni(train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_cat_features(df):\n",
    "    cat_features = []\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == 'object':\n",
    "            cat_features.append(col)\n",
    "    #cat_features = [x for x in cat_features if x not in ('fieldPosition','StadiumType','GameWeather')]\n",
    "    cat_features.append('NflId')\n",
    "    df = df.drop(cat_features, axis=1)\n",
    "    return df\n",
    "\n",
    "# 找每行是unique值的column;player_col里面就存了这些columns\n",
    "def find_uni_col(df):\n",
    "    uni_col = []\n",
    "    for col in df.columns:\n",
    "        if df[col][:11].unique().shape[0]!=1:\n",
    "            uni_col.append(col)\n",
    "    uni_col.append('PlayId')\n",
    "    return uni_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(df):\n",
    "    df.fillna(-999, inplace=True)\n",
    "    #inplace : boolean, 默认值 False。如果为Ture,在原地填满。\n",
    "    #注意：这将修改次对象上的任何其他视图（例如，DataFrame中的列的无复制贴片）\n",
    "    \n",
    "    #添加X,Y的spread:    \n",
    "    team_spread=df.groupby(['PlayId','Team'],as_index=False).agg({'X':['std'],'Y':['std']})\n",
    "    team_spread.columns=['PlayId','Team','team_Xspread','team_Yspread']\n",
    "    df=pd.merge(df,team_spread,how=\"inner\",on=['PlayId','Team'])\n",
    "    \n",
    "    #按照offense和defense分组\n",
    "    df_offense = df[(df['Offense']=='offense')]\n",
    "    df_defense = df[(df['Offense']=='defense')]\n",
    "    \n",
    "    #light GBM可以直接使用categorical features，所以是不是不用删掉categorical features?\n",
    "    df_offense = remove_cat_features(df_offense)\n",
    "    df_defense = remove_cat_features(df_defense)\n",
    "    \n",
    "    uni_a = find_uni_col(df_offense)\n",
    "    uni_a.remove('JerseyNumber')\n",
    "    uni_a = [x for x in uni_a if x not in ('YardLine','Down','Distance','Yards')]\n",
    "    \n",
    "    df_unique_offense = df_offense[uni_a]\n",
    "    df_unique_defense = df_defense[uni_a]\n",
    "    \n",
    "    uni_a.remove('PlayId')\n",
    "    df_no_unique_offense = df_offense.drop(uni_a+['JerseyNumber','GameId'], axis=1)\n",
    "    df_no_unique_defense = df_defense.drop(uni_a+['JerseyNumber','GameId'], axis=1)\n",
    "    \n",
    "    #example_unique=example_unique.drop(['PlayId'],axis=1)\n",
    "    \n",
    "    # 注意这里的player_num其实是必要的\n",
    "    df_uni_piv_offense = df_unique_offense.pivot(index='PlayId', columns='player_num')\n",
    "    df_uni_piv_defense = df_unique_defense.pivot(index='PlayId', columns='player_num')\n",
    "    \n",
    "    df_no_unique_offense = df_no_unique_offense.drop_duplicates(subset='PlayId')\n",
    "\n",
    "    df_no_unique_defense = df_no_unique_defense.drop_duplicates(subset='PlayId')\n",
    "    \n",
    "    df_clean_offense = pd.merge(df_uni_piv_offense,df_no_unique_offense,how='inner',on='PlayId')\n",
    "    #不需要df_clean_defense=pd.merge(df_uni_piv_defense,df_no_unique_defense,how='inner',on'PlayId)因为\n",
    "    #如果这么做的话，后面再做df_clean的时候会重复列出场地信息\n",
    "    \n",
    "    df_clean = pd.merge(df_clean_offense,df_uni_piv_defense,how='inner',on='PlayId')\n",
    "    \n",
    "    return df_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/pandas/core/reshape/merge.py:617: UserWarning: merging between different levels can give an unintended result (2 levels on the left, 1 on the right)\n",
      "  warnings.warn(msg, UserWarning)\n",
      "/opt/conda/lib/python3.6/site-packages/pandas/core/reshape/merge.py:617: UserWarning: merging between different levels can give an unintended result (1 levels on the left, 2 on the right)\n",
      "  warnings.warn(msg, UserWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PlayId</th>\n",
       "      <th>('X', 1)_x</th>\n",
       "      <th>('X', 2)_x</th>\n",
       "      <th>('X', 3)_x</th>\n",
       "      <th>('X', 4)_x</th>\n",
       "      <th>('X', 5)_x</th>\n",
       "      <th>('X', 6)_x</th>\n",
       "      <th>('X', 7)_x</th>\n",
       "      <th>('X', 8)_x</th>\n",
       "      <th>('X', 9)_x</th>\n",
       "      <th>...</th>\n",
       "      <th>(PlayerBMI_y, 2)</th>\n",
       "      <th>(PlayerBMI_y, 3)</th>\n",
       "      <th>(PlayerBMI_y, 4)</th>\n",
       "      <th>(PlayerBMI_y, 5)</th>\n",
       "      <th>(PlayerBMI_y, 6)</th>\n",
       "      <th>(PlayerBMI_y, 7)</th>\n",
       "      <th>(PlayerBMI_y, 8)</th>\n",
       "      <th>(PlayerBMI_y, 9)</th>\n",
       "      <th>(PlayerBMI_y, 10)</th>\n",
       "      <th>(PlayerBMI_y, 11)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20170907000118</td>\n",
       "      <td>75.82</td>\n",
       "      <td>74.78</td>\n",
       "      <td>75.43</td>\n",
       "      <td>75.90</td>\n",
       "      <td>79.76</td>\n",
       "      <td>76.47</td>\n",
       "      <td>74.70</td>\n",
       "      <td>78.75</td>\n",
       "      <td>74.60</td>\n",
       "      <td>...</td>\n",
       "      <td>35.993600</td>\n",
       "      <td>33.744000</td>\n",
       "      <td>30.619556</td>\n",
       "      <td>27.935571</td>\n",
       "      <td>32.351351</td>\n",
       "      <td>26.496727</td>\n",
       "      <td>26.172647</td>\n",
       "      <td>26.702703</td>\n",
       "      <td>26.715085</td>\n",
       "      <td>35.589086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20170907000139</td>\n",
       "      <td>70.54</td>\n",
       "      <td>66.75</td>\n",
       "      <td>66.19</td>\n",
       "      <td>67.40</td>\n",
       "      <td>71.79</td>\n",
       "      <td>66.92</td>\n",
       "      <td>67.09</td>\n",
       "      <td>71.07</td>\n",
       "      <td>66.46</td>\n",
       "      <td>...</td>\n",
       "      <td>35.993600</td>\n",
       "      <td>33.744000</td>\n",
       "      <td>30.619556</td>\n",
       "      <td>27.935571</td>\n",
       "      <td>32.351351</td>\n",
       "      <td>26.496727</td>\n",
       "      <td>26.172647</td>\n",
       "      <td>26.702703</td>\n",
       "      <td>26.715085</td>\n",
       "      <td>35.589086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20170907000189</td>\n",
       "      <td>45.12</td>\n",
       "      <td>44.22</td>\n",
       "      <td>44.81</td>\n",
       "      <td>45.24</td>\n",
       "      <td>49.51</td>\n",
       "      <td>43.83</td>\n",
       "      <td>43.36</td>\n",
       "      <td>48.66</td>\n",
       "      <td>43.16</td>\n",
       "      <td>...</td>\n",
       "      <td>35.993600</td>\n",
       "      <td>33.744000</td>\n",
       "      <td>30.619556</td>\n",
       "      <td>27.935571</td>\n",
       "      <td>32.351351</td>\n",
       "      <td>26.496727</td>\n",
       "      <td>26.172647</td>\n",
       "      <td>26.702703</td>\n",
       "      <td>26.715085</td>\n",
       "      <td>35.589086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20170907000345</td>\n",
       "      <td>11.18</td>\n",
       "      <td>10.92</td>\n",
       "      <td>11.56</td>\n",
       "      <td>16.30</td>\n",
       "      <td>12.64</td>\n",
       "      <td>11.76</td>\n",
       "      <td>11.47</td>\n",
       "      <td>15.53</td>\n",
       "      <td>11.49</td>\n",
       "      <td>...</td>\n",
       "      <td>28.749228</td>\n",
       "      <td>35.993600</td>\n",
       "      <td>33.744000</td>\n",
       "      <td>30.619556</td>\n",
       "      <td>31.744356</td>\n",
       "      <td>27.935571</td>\n",
       "      <td>40.439189</td>\n",
       "      <td>30.425676</td>\n",
       "      <td>26.715085</td>\n",
       "      <td>39.412162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20170907000395</td>\n",
       "      <td>29.51</td>\n",
       "      <td>33.19</td>\n",
       "      <td>33.34</td>\n",
       "      <td>31.36</td>\n",
       "      <td>34.25</td>\n",
       "      <td>33.61</td>\n",
       "      <td>35.07</td>\n",
       "      <td>36.19</td>\n",
       "      <td>34.18</td>\n",
       "      <td>...</td>\n",
       "      <td>27.976531</td>\n",
       "      <td>40.442143</td>\n",
       "      <td>32.494222</td>\n",
       "      <td>26.647776</td>\n",
       "      <td>27.043535</td>\n",
       "      <td>31.244444</td>\n",
       "      <td>26.496727</td>\n",
       "      <td>34.020270</td>\n",
       "      <td>29.983138</td>\n",
       "      <td>41.081081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23166</th>\n",
       "      <td>20181230153910</td>\n",
       "      <td>46.82</td>\n",
       "      <td>45.03</td>\n",
       "      <td>46.38</td>\n",
       "      <td>46.23</td>\n",
       "      <td>46.27</td>\n",
       "      <td>49.57</td>\n",
       "      <td>44.56</td>\n",
       "      <td>49.77</td>\n",
       "      <td>44.21</td>\n",
       "      <td>...</td>\n",
       "      <td>32.817515</td>\n",
       "      <td>28.362732</td>\n",
       "      <td>27.863889</td>\n",
       "      <td>32.494222</td>\n",
       "      <td>28.362732</td>\n",
       "      <td>38.243200</td>\n",
       "      <td>35.904605</td>\n",
       "      <td>28.494652</td>\n",
       "      <td>25.370489</td>\n",
       "      <td>43.231502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23167</th>\n",
       "      <td>20181230154035</td>\n",
       "      <td>32.97</td>\n",
       "      <td>33.45</td>\n",
       "      <td>35.16</td>\n",
       "      <td>30.03</td>\n",
       "      <td>34.72</td>\n",
       "      <td>34.73</td>\n",
       "      <td>31.00</td>\n",
       "      <td>35.12</td>\n",
       "      <td>34.96</td>\n",
       "      <td>...</td>\n",
       "      <td>26.779696</td>\n",
       "      <td>28.728030</td>\n",
       "      <td>31.420982</td>\n",
       "      <td>27.043535</td>\n",
       "      <td>27.835053</td>\n",
       "      <td>27.043535</td>\n",
       "      <td>36.163771</td>\n",
       "      <td>31.157895</td>\n",
       "      <td>31.001126</td>\n",
       "      <td>27.976531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23168</th>\n",
       "      <td>20181230154082</td>\n",
       "      <td>42.00</td>\n",
       "      <td>43.26</td>\n",
       "      <td>40.99</td>\n",
       "      <td>39.45</td>\n",
       "      <td>42.53</td>\n",
       "      <td>43.88</td>\n",
       "      <td>39.93</td>\n",
       "      <td>43.81</td>\n",
       "      <td>43.84</td>\n",
       "      <td>...</td>\n",
       "      <td>26.779696</td>\n",
       "      <td>28.728030</td>\n",
       "      <td>31.420982</td>\n",
       "      <td>27.043535</td>\n",
       "      <td>27.835053</td>\n",
       "      <td>27.043535</td>\n",
       "      <td>36.163771</td>\n",
       "      <td>31.157895</td>\n",
       "      <td>31.001126</td>\n",
       "      <td>27.976531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23169</th>\n",
       "      <td>20181230154135</td>\n",
       "      <td>83.49</td>\n",
       "      <td>84.06</td>\n",
       "      <td>82.20</td>\n",
       "      <td>80.62</td>\n",
       "      <td>84.09</td>\n",
       "      <td>84.23</td>\n",
       "      <td>84.96</td>\n",
       "      <td>84.90</td>\n",
       "      <td>84.19</td>\n",
       "      <td>...</td>\n",
       "      <td>26.779696</td>\n",
       "      <td>28.728030</td>\n",
       "      <td>31.420982</td>\n",
       "      <td>27.043535</td>\n",
       "      <td>27.835053</td>\n",
       "      <td>27.043535</td>\n",
       "      <td>41.367644</td>\n",
       "      <td>31.157895</td>\n",
       "      <td>31.001126</td>\n",
       "      <td>27.976531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23170</th>\n",
       "      <td>20181230154157</td>\n",
       "      <td>85.12</td>\n",
       "      <td>86.76</td>\n",
       "      <td>81.82</td>\n",
       "      <td>87.34</td>\n",
       "      <td>86.00</td>\n",
       "      <td>85.68</td>\n",
       "      <td>86.77</td>\n",
       "      <td>86.76</td>\n",
       "      <td>87.26</td>\n",
       "      <td>...</td>\n",
       "      <td>26.779696</td>\n",
       "      <td>28.728030</td>\n",
       "      <td>31.420982</td>\n",
       "      <td>30.297297</td>\n",
       "      <td>27.043535</td>\n",
       "      <td>27.835053</td>\n",
       "      <td>41.367644</td>\n",
       "      <td>31.157895</td>\n",
       "      <td>31.001126</td>\n",
       "      <td>27.976531</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>23171 rows × 250 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               PlayId  ('X', 1)_x  ('X', 2)_x  ('X', 3)_x  ('X', 4)_x  \\\n",
       "0      20170907000118       75.82       74.78       75.43       75.90   \n",
       "1      20170907000139       70.54       66.75       66.19       67.40   \n",
       "2      20170907000189       45.12       44.22       44.81       45.24   \n",
       "3      20170907000345       11.18       10.92       11.56       16.30   \n",
       "4      20170907000395       29.51       33.19       33.34       31.36   \n",
       "...               ...         ...         ...         ...         ...   \n",
       "23166  20181230153910       46.82       45.03       46.38       46.23   \n",
       "23167  20181230154035       32.97       33.45       35.16       30.03   \n",
       "23168  20181230154082       42.00       43.26       40.99       39.45   \n",
       "23169  20181230154135       83.49       84.06       82.20       80.62   \n",
       "23170  20181230154157       85.12       86.76       81.82       87.34   \n",
       "\n",
       "       ('X', 5)_x  ('X', 6)_x  ('X', 7)_x  ('X', 8)_x  ('X', 9)_x  ...  \\\n",
       "0           79.76       76.47       74.70       78.75       74.60  ...   \n",
       "1           71.79       66.92       67.09       71.07       66.46  ...   \n",
       "2           49.51       43.83       43.36       48.66       43.16  ...   \n",
       "3           12.64       11.76       11.47       15.53       11.49  ...   \n",
       "4           34.25       33.61       35.07       36.19       34.18  ...   \n",
       "...           ...         ...         ...         ...         ...  ...   \n",
       "23166       46.27       49.57       44.56       49.77       44.21  ...   \n",
       "23167       34.72       34.73       31.00       35.12       34.96  ...   \n",
       "23168       42.53       43.88       39.93       43.81       43.84  ...   \n",
       "23169       84.09       84.23       84.96       84.90       84.19  ...   \n",
       "23170       86.00       85.68       86.77       86.76       87.26  ...   \n",
       "\n",
       "       (PlayerBMI_y, 2)  (PlayerBMI_y, 3)  (PlayerBMI_y, 4)  (PlayerBMI_y, 5)  \\\n",
       "0             35.993600         33.744000         30.619556         27.935571   \n",
       "1             35.993600         33.744000         30.619556         27.935571   \n",
       "2             35.993600         33.744000         30.619556         27.935571   \n",
       "3             28.749228         35.993600         33.744000         30.619556   \n",
       "4             27.976531         40.442143         32.494222         26.647776   \n",
       "...                 ...               ...               ...               ...   \n",
       "23166         32.817515         28.362732         27.863889         32.494222   \n",
       "23167         26.779696         28.728030         31.420982         27.043535   \n",
       "23168         26.779696         28.728030         31.420982         27.043535   \n",
       "23169         26.779696         28.728030         31.420982         27.043535   \n",
       "23170         26.779696         28.728030         31.420982         30.297297   \n",
       "\n",
       "       (PlayerBMI_y, 6)  (PlayerBMI_y, 7)  (PlayerBMI_y, 8)  (PlayerBMI_y, 9)  \\\n",
       "0             32.351351         26.496727         26.172647         26.702703   \n",
       "1             32.351351         26.496727         26.172647         26.702703   \n",
       "2             32.351351         26.496727         26.172647         26.702703   \n",
       "3             31.744356         27.935571         40.439189         30.425676   \n",
       "4             27.043535         31.244444         26.496727         34.020270   \n",
       "...                 ...               ...               ...               ...   \n",
       "23166         28.362732         38.243200         35.904605         28.494652   \n",
       "23167         27.835053         27.043535         36.163771         31.157895   \n",
       "23168         27.835053         27.043535         36.163771         31.157895   \n",
       "23169         27.835053         27.043535         41.367644         31.157895   \n",
       "23170         27.043535         27.835053         41.367644         31.157895   \n",
       "\n",
       "       (PlayerBMI_y, 10)  (PlayerBMI_y, 11)  \n",
       "0              26.715085          35.589086  \n",
       "1              26.715085          35.589086  \n",
       "2              26.715085          35.589086  \n",
       "3              26.715085          39.412162  \n",
       "4              29.983138          41.081081  \n",
       "...                  ...                ...  \n",
       "23166          25.370489          43.231502  \n",
       "23167          31.001126          27.976531  \n",
       "23168          31.001126          27.976531  \n",
       "23169          31.001126          27.976531  \n",
       "23170          31.001126          27.976531  \n",
       "\n",
       "[23171 rows x 250 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = clean_data(train)\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_columns=train.columns.values.tolist() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_columns\n",
    "all_columns.remove('PlayId')\n",
    "all_columns.remove('Yards')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 要先把X和Y分开\n",
    "X_train=pd.DataFrame(data=train,columns=all_columns)\n",
    "y_train = np.array([train['Yards'][i] for i in range(0,23171)])\n",
    "\n",
    "#scaler = StandardScaler()\n",
    "#X_train = scaler.fit_transform(X_train)\n",
    "y = y_train\n",
    "target = y[np.arange(0, len(train), 22)]\n",
    "standard_deviation = np.std(target)\n",
    "scaler = StandardScaler()\n",
    "# 去掉categorical features的一个原因就是，scale的时候不去掉会有问题\n",
    "X_train = scaler.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/model_selection/_split.py:657: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of members in any class cannot be less than n_splits=5.\n",
      "  % (min_groups, self.n_splits)), Warning)\n",
      "/opt/conda/lib/python3.6/site-packages/lightgbm/engine.py:148: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 200 rounds\n",
      "[50]\ttraining's l1: 3.20714\tvalid_1's l1: 3.91371\n",
      "[100]\ttraining's l1: 2.81316\tvalid_1's l1: 3.97442\n",
      "[150]\ttraining's l1: 2.4808\tvalid_1's l1: 4.03641\n",
      "[200]\ttraining's l1: 2.19182\tvalid_1's l1: 4.06697\n",
      "Early stopping, best iteration is:\n",
      "[11]\ttraining's l1: 3.47799\tvalid_1's l1: 3.71743\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[50]\ttraining's l1: 3.22353\tvalid_1's l1: 3.87793\n",
      "[100]\ttraining's l1: 2.83714\tvalid_1's l1: 3.94461\n",
      "[150]\ttraining's l1: 2.49639\tvalid_1's l1: 3.97053\n",
      "[200]\ttraining's l1: 2.20744\tvalid_1's l1: 3.99879\n",
      "Early stopping, best iteration is:\n",
      "[13]\ttraining's l1: 3.45819\tvalid_1's l1: 3.67678\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[50]\ttraining's l1: 3.24846\tvalid_1's l1: 3.80204\n",
      "[100]\ttraining's l1: 2.84237\tvalid_1's l1: 3.86437\n",
      "[150]\ttraining's l1: 2.50672\tvalid_1's l1: 3.90114\n",
      "[200]\ttraining's l1: 2.22023\tvalid_1's l1: 3.94087\n",
      "Early stopping, best iteration is:\n",
      "[12]\ttraining's l1: 3.48501\tvalid_1's l1: 3.61283\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[50]\ttraining's l1: 3.26708\tvalid_1's l1: 3.73966\n",
      "[100]\ttraining's l1: 2.87311\tvalid_1's l1: 3.81408\n",
      "[150]\ttraining's l1: 2.54317\tvalid_1's l1: 3.87913\n",
      "[200]\ttraining's l1: 2.24496\tvalid_1's l1: 3.90925\n",
      "Early stopping, best iteration is:\n",
      "[12]\ttraining's l1: 3.51288\tvalid_1's l1: 3.54613\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[50]\ttraining's l1: 3.26058\tvalid_1's l1: 3.78514\n",
      "[100]\ttraining's l1: 2.86636\tvalid_1's l1: 3.84992\n",
      "[150]\ttraining's l1: 2.53185\tvalid_1's l1: 3.8903\n",
      "[200]\ttraining's l1: 2.23682\tvalid_1's l1: 3.93937\n",
      "Early stopping, best iteration is:\n",
      "[13]\ttraining's l1: 3.4918\tvalid_1's l1: 3.5748\n",
      "[<lightgbm.basic.Booster object at 0x7f844f38f2e8>, <lightgbm.basic.Booster object at 0x7f844f38f320>, <lightgbm.basic.Booster object at 0x7f844abcc9e8>, <lightgbm.basic.Booster object at 0x7f844abcc6d8>, <lightgbm.basic.Booster object at 0x7f844abccf60>]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "params = {\n",
    "            'boosting_type': 'gbdt',\n",
    "            'objective': 'regression',\n",
    "            'metric': 'mae',\n",
    "            'learning_rate': 0.1,\n",
    "            'num_iterations': 500,\n",
    "            'verbosity': -1, \n",
    "            \"boost_from_average\" : False,\n",
    "            'num_leaves': 44,\n",
    "            'bagging_fraction': 0.8,\n",
    "            'bagging_freq': 3,\n",
    "            'min_child_samples': 43,\n",
    "            'n_estimators': 300,\n",
    "            'feature_fraction': 0.9,\n",
    "            'lambda_l1': 0.13413394854686794,  \n",
    "            'lambda_l2': 0.0009122197743451751,\n",
    "            'random_state': 42\n",
    "            }\n",
    "\n",
    "folds = 5\n",
    "seed = 999\n",
    "\n",
    "kf = StratifiedKFold(n_splits=folds, shuffle=True, random_state=seed)\n",
    "\n",
    "models = []\n",
    "\n",
    "for train_index, val_index in kf.split(X_train, y_train):\n",
    "    train_X = X_train[train_index]\n",
    "    val_X = X_train[val_index]\n",
    "    train_y = y_train[train_index]\n",
    "    val_y = y_train[val_index]\n",
    "    lgb_train = lgb.Dataset(train_X, train_y)\n",
    "    lgb_eval = lgb.Dataset(val_X, val_y)\n",
    "    gbm = lgb.train(params,\n",
    "                lgb_train,\n",
    "                num_boost_round=50,\n",
    "                valid_sets=(lgb_train, lgb_eval),\n",
    "                early_stopping_rounds=200,\n",
    "                verbose_eval = 50)\n",
    "    models.append(gbm)\n",
    "\n",
    "print(models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12/2/2019 对怎么调参数结果都不变这个问题的研究\n",
    "import scipy\n",
    "# 首先搞一个自己的test_df:\n",
    "fake_test_df = X_train[:1]\n",
    "\n",
    "fake_y_pred = np.mean([model.predict(fake_test_df, num_iteration=model.best_iteration) for model in models],axis=0)\n",
    "#fake_y_pred = np.clip(np.cumsum(fake_y_pred, axis=1), 0, 1).tolist()[0]\n",
    "\n",
    "fake_pred_df = np.zeros((1, 199))  # 1是number of rows, 199是number of columns\n",
    "current_cdf = scipy.stats.norm(loc = fake_y_pred, scale = standard_deviation).cdf(-98)\n",
    "fake_pred_df[0][1]\n",
    "\n",
    "for A in range(len(fake_pred_df[0])):\n",
    "    current_cdf = scipy.stats.norm(loc = fake_y_pred, scale = standard_deviation).cdf(A-99)\n",
    "    fake_pred_df[0][A] = current_cdf\n",
    "\n",
    "len(current_cdf)\n",
    "    \n",
    "fake_final_pred_df = pd.DataFrame(data=fake_pred_df)\n",
    "fake_final_pred_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "num iteration: 1000\n",
    "num of fold = 5\n",
    "early stop = 300\n",
    "training's l1: 3.4802\tvalid_1's l1: 3.51941\n",
    "\n",
    "num iteration: 500\n",
    "fold = 5\n",
    "early story = 200\n",
    "training's l1: 3.4802\tvalid_1's l1: 3.51941\n",
    "\n",
    "num iteration: 500\n",
    "fold = 10\n",
    "early stop = 200\n",
    "training's l1: 3.47776\tvalid_1's l1: 3.50066\n",
    "training's l1: 3.48447\tvalid_1's l1: 3.4561\n",
    "training's l1: 3.47314\tvalid_1's l1: 3.49199\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最早的参数：\n",
    "params = {\n",
    "            'boosting_type': 'gbdt',\n",
    "            'objective': 'regression',\n",
    "            'metric': {'l2_root'},\n",
    "            'subsample': 0.25,\n",
    "            'subsample_freq': 1,\n",
    "            'learning_rate': 0.1,\n",
    "            'num_iterations': 500,\n",
    "            'num_leaves': 31,\n",
    "            'feature_fraction': 0.8,\n",
    "            'lambda_l1': 1,  \n",
    "            'lambda_l2': 1\n",
    "            }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11/25/2019 3:20pm 改动之前的参数：\n",
    "params = {\n",
    "            'boosting_type': 'gbdt',\n",
    "            'objective': 'regression',\n",
    "            'metric': 'mae',\n",
    "            'learning_rate': 0.005,\n",
    "            'num_iterations': 500,\n",
    "            'verbosity': -1, \n",
    "            \"boost_from_average\" : False,\n",
    "            'num_leaves': 44,\n",
    "            'bagging_fraction': 0.9999128827046064,\n",
    "            'bagging_freq': 3,\n",
    "            'min_child_samples': 43,\n",
    "            'n_estimators': 300,\n",
    "            'feature_fraction': 0.4271070738920401,\n",
    "            'lambda_l1': 0.13413394854686794,  \n",
    "            'lambda_l2': 0.0009122197743451751,\n",
    "            'random_state': 42\n",
    "            }\n",
    "改动过的参数结果较好\n",
    "\n",
    "11/25/2019 3:51pm \n",
    "尝试用multiclass这个objective funciton; 同时设置number of class:199，改变metric为multi logloss;\n",
    "出现错误：Label must be in [0, 199), but found -4 in label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3438it [13:02,  4.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your submission file has been saved!  Once you `Commit` your Notebook and it finishes running, you can submit the file to the competition from the Notebook Viewer `Output` tab.\n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "import scipy\n",
    "import scipy.stats as st\n",
    "    \n",
    "batch_no = 0\n",
    "for (test_df, sample_prediction_df) in tqdm.tqdm(env.iter_test()):\n",
    "    try:\n",
    "        dist_to_end_test = test_df.apply(lambda x:(100 - x.loc['YardLine']) if x.loc['own_field']==1 else x.loc['YardLine'],axis=1)\n",
    "        test_df['WindSpeed'] = test_df['WindSpeed'].apply(lambda x: x.lower().replace('mph', '').strip() if not pd.isna(x) else x)\n",
    "        test_df['WindSpeed'] = test_df['WindSpeed'].apply(lambda x: (int(x.split('-')[0])+int(x.split('-')[1]))/2 if not pd.isna(x) and '-' in x else x)\n",
    "        test_df['WindSpeed'] = test_df['WindSpeed'].apply(lambda x: (int(x.split()[0])+int(x.split()[-1]))/2 if not pd.isna(x) and type(x)!=float and 'gusts up to' in x else x)\n",
    "        test_df['WindSpeed'] = test_df['WindSpeed'].apply(str_to_float)\n",
    "    \n",
    "        test_df['WindDirection'] = test_df['WindDirection'].apply(clean_WindDirection)\n",
    "        test_df['WindDirection'] = test_df['WindDirection'].apply(transform_WindDirection)\n",
    "    \n",
    "        Turf = {'Field Turf':'Artificial', 'A-Turf Titan':'Artificial', 'Grass':'Natural', 'UBU Sports Speed S5-M':'Artificial', \n",
    "        'Artificial':'Artificial', 'DD GrassMaster':'Artificial', 'Natural Grass':'Natural', \n",
    "        'UBU Speed Series-S5-M':'Artificial', 'FieldTurf':'Artificial', 'FieldTurf 360':'Artificial', 'Natural grass':'Natural', 'grass':'Natural', \n",
    "        'Natural':'Natural', 'Artifical':'Artificial', 'FieldTurf360':'Artificial', 'Naturall Grass':'Natural', 'Field turf':'Artificial', \n",
    "        'SISGrass':'Artificial', 'Twenty-Four/Seven Turf':'Artificial', 'natural grass':'Natural'} \n",
    "\n",
    "        test_df['Turf'] = test_df['Turf'].map(Turf)\n",
    "        test_df['Turf'] = test_df['Turf'] == 'Natural'\n",
    "\n",
    "    # solve team name encoding problem\n",
    "        map_abbr = {'ARI': 'ARZ', 'BAL': 'BLT', 'CLE': 'CLV', 'HOU': 'HST'}\n",
    "        for abb in test_df['PossessionTeam'].unique():\n",
    "            map_abbr[abb] = abb\n",
    "    \n",
    "        test_df['PossessionTeam'] = test_df['PossessionTeam'].map(map_abbr)\n",
    "        test_df['HomeTeamAbbr'] = test_df['HomeTeamAbbr'].map(map_abbr)\n",
    "        test_df['VisitorTeamAbbr'] = test_df['VisitorTeamAbbr'].map(map_abbr)\n",
    "\n",
    "    # Before pivot:\n",
    "    # Creat: IsBallCarrier, ToLeft, std_x, std_y, offense, age, bmi, player number\n",
    "    #train['IsBallCarrier'] = train['NflId'] == train['NflIdRusher']\n",
    "        test_df['ToLeft'] = test_df['PlayDirection'] == 'left'\n",
    "    \n",
    "        test_df = define_offense(test_df)\n",
    "        test_df = bye_XY(test_df)\n",
    "        test_df = dis_max(test_df)\n",
    "        test_df = define_team_average_age(test_df)\n",
    "    \n",
    "        test_df = define_bmi(test_df)\n",
    "        append_player_number(test_df)\n",
    "        test_df = define_Top10UniversityAlumni(test_df)\n",
    "    \n",
    "    ## final test data:\n",
    "        test_df = clean_data(test_df)\n",
    "        test_df = scaler.fit_transform(test_df)\n",
    "        y_pred = np.mean([model.predict(test_df, num_iteration=model.best_iteration) for model in models],axis=0)\n",
    "        #y_pred = np.clip(np.cumsum(y_pred, axis=1), 0, 1).tolist()[0] #这一行好像有问题\n",
    "        \n",
    "        #y_pred_p = models.predict(test_df)\n",
    "        #y_pred_first = y_pred_p[0]\n",
    "        \n",
    "    except:\n",
    "        #y_pred_first = 1\n",
    "        y_pred = 1\n",
    "\n",
    "    pred_df = np.zeros((1, 199))  \n",
    "    for A in range(len(pred_df[0])):\n",
    "        current_cdf = scipy.stats.norm(loc = y_pred, scale = standard_deviation).cdf(A-99)\n",
    "        pred_df[0][A] = current_cdf\n",
    "        \n",
    "   #pred_df[0][:80] = 0\n",
    "\n",
    "    final_pred_df = pd.DataFrame(data=pred_df, columns=sample_prediction_df.columns)\n",
    "    env.predict(final_pred_df)\n",
    "    batch_no += 1\n",
    "\n",
    "env.write_submission_file()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def get_score(y_pred,cdf,w,dist_to_end):\n",
    "    y_pred = int(y_pred)\n",
    "    if y_pred ==w:\n",
    "        y_pred_array = cdf.copy()\n",
    "    elif y_pred - w >0:\n",
    "        y_pred_array = np.zeros(199)\n",
    "        y_pred_array[(y_pred-w):] = cdf[:(-(y_pred-w))].copy()\n",
    "    elif w - y_pred >0:\n",
    "        y_pred_array = np.ones(199)\n",
    "        y_pred_array[:(y_pred-w)] = cdf[(w-y_pred):].copy()\n",
    "    y_pred_array[-1]=1\n",
    "    y_pred_array[(dist_to_end+99):]=1\n",
    "    return y_pred_array\n",
    "\n",
    "dist_to_end_test = test_df.apply(lambda x:(100 - x.loc['YardLine']) if x.loc['own_field']==1 else x.loc['YardLine'],axis=1)\n",
    "\n",
    "pred_value = 0\n",
    "for model in models:\n",
    "    pred_value += model.predict(X_test)[0]/5\n",
    "pred_data = list(get_score(pred_value,cdf,4,dist_to_end_test.values[0]))\n",
    "pred_data = np.array(pred_data).reshape(1,199)\n",
    "pred_target = pd.DataFrame(index = sample_prediction_df.index, \\\n",
    "                               columns = sample_prediction_df.columns, \\\n",
    "                               #data = np.array(pred_data))\n",
    "                               data = pred_data)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
